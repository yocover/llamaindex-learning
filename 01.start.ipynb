{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connected to .env (Python 3.10.16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 什么是 LlamaIndex？`\n",
    "\n",
    "`LlamaIndex` 是一个用于 LLM 应用程序的数据框架，用于注入，结构化，并访问私有或特定领域数据。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LlamaIndex 是一个数据框架，用于构建 LLM（大型语言模型）应用程序。\n",
    "\n",
    "它提供了一套工具，帮助开发者连接自定义数据源与大型语言模型，从而创建能够利用私有或特定领域数据的智能应用\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 大核心工具\n",
    "\n",
    "![llamaindex应运而生](./images/image.png)\n",
    "\n",
    "`LlamaIndex` 提供了 5 大核心工具：\n",
    "\n",
    "- Data connectors\n",
    "- Data indexes\n",
    "- Engines\n",
    "- Data agents\n",
    "- Application integrations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data Connectors（数据连接器），数据连接器用于从各种来源加载数据到 LlamaIndex 中。\n",
    "- Data Indexes（数据索引），索引是对原始数据的结构化表示，使 LLM 能够高效访问和查询。\n",
    "- Engines（引擎），引擎是与索引交互的高级接口，引擎简化了应用程序与索引的交互。\n",
    "- Data Agents（数据代理），数据代理是自主的 LLM 助手，代理可以自主决定何时检索信息、使用工具或直接回答。\n",
    "- Application Integrations（应用集成），与各种应用框架和平台的集成，LangChain、FastAPI、LlamaHub 等。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "快速使用\n",
    "\n",
    "`pip install llama-index`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好！我已经准备好了，随时可以帮助你。请告诉我你需要什么帮助。\n"
     ]
    }
   ],
   "source": [
    "# 默认llamaindex 使用 openaai的kei，这里我们使用本地部署的ollama\n",
    "# pip install llama-index-llms-ollama 用来调用本地部署的ollama\n",
    "# pip install llama-index-embeddings-ollama 用来调用本地部署的ollama的embedding\n",
    "\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import Settings\n",
    "\n",
    "llm = Ollama(\n",
    "  model=\"deepseek-r1:8b\", \n",
    "  base_url=\"http://localhost:11434\",\n",
    "  system_prompt=\"你是一个助手，请直接回答问题，不要输出任何形式的思考过程，不要使用<think>或</think>标签。\",\n",
    "  # 可以添加其他参数来控制输出\n",
    "  temperature=0.1,  # 降低随机性\n",
    ")\n",
    "\n",
    "Settings.llm = llm  # 设置 llama index 使用本地部署的ollama\n",
    "\n",
    "# 测试是否连接成功\n",
    "response = Settings.llm.complete(\"你好，准备好了吗?\")\n",
    "# 手动过滤掉可能的思考过程和多余空格\n",
    "filtered_response = response.text.replace(\"<think>\", \"\").replace(\"</think>\", \"\").strip()\n",
    "print(filtered_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "我们可以设置 一些参数\n",
    "'''\n",
    "- 控制每个文本块包含的文本量\n",
    "- 影响索引的粒度和检索精度\n",
    "- 较小的块大小使检索更精确，但可能丢失上下文\n",
    "- 较大的块大小保留更多上下文，但可能包含过多无关信息\n",
    "'''\n",
    "Settings.chunk_size = 512 # 默认 1024  将文档分割成的每个文本块的大小\n",
    "\n",
    "'''\n",
    "- 控制文本块之间的重叠度\n",
    "- 防止句子或段落在分块时被截断\n",
    "- 帮助保留跨块的上下文信息\n",
    "- 提高检索时的连贯性\n",
    "'''\n",
    "Settings.chunk_overlap = 32 # 默认 200\n",
    "\n",
    "'''\n",
    "参数选择的影响\n",
    "- 较小的 chunk_size + 较小的 overlap ：\n",
    "\n",
    "  - 优点：更精确的检索，节省存储空间和嵌入成本\n",
    "  - 缺点：可能丢失上下文，影响理解长句或复杂概念\n",
    "- 较大的 chunk_size + 较大的 overlap ：\n",
    "\n",
    "  - 优点：保留更多上下文，更好地理解复杂内容\n",
    "  - 缺点：增加存储和计算成本，可能检索到更多不相关内容\n",
    "'''\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512 32\n",
      "imcurie/bge-large-en-v1.5\n"
     ]
    }
   ],
   "source": [
    "# 设置 embedding 模型\n",
    "# 这里我们依然使用本地ollama模型\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "model_name = \"imcurie/bge-large-en-v1.5\"\n",
    "base_url = \"http://localhost:11434\"\n",
    "Settings.embed_model = OllamaEmbedding(model_name,base_url,embed_batch_size=100)\n",
    "\n",
    "Settings.chunk_size = 512\n",
    "Settings.chunk_overlap = 32\n",
    "\n",
    "print(Settings.chunk_size, Settings.chunk_overlap)\n",
    "print(Settings.embed_model.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "首先，我需要理解用户的问题：“首开股份因何被中国证监会处罚？”根据提供的上下文信息，首开股份在2021年存货减值测试中高估了子公司存货可变现净值，导致少计提存货跌价准备和资产减值损失4.05亿元。这一错误使得公司在2022年5月至2023年5月期间的发行文件和银行间债券市场披露的年度报告中存在错报。因此，中国证监会对首开股份进行了行政处罚，包括警告并处以150万元的罚款。\n",
      "</think>\n",
      "\n",
      "首开股份因在2021年存货减值测试中高估下属子公司存货可变现净值，少计提存货跌价准备和资产减值损失4.05亿元，导致错报。\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "# 加载文档\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "\n",
    "# 创建索引\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# 创建查询引擎\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "# 查询数据\n",
    "response = query_engine.query(\"首开股份因何被中国证监会处罚？\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
