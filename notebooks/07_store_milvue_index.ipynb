{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "已连接到 .env (Python 3.10.16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "\n",
    "from src.llms.siliconflow import SiliconflowLLM\n",
    "api_key = \"key\"\n",
    "\n",
    "llm = SiliconflowLLM(\n",
    "    api_key=api_key,\n",
    "    api_base=\"https://api.siliconflow.cn/v1\",\n",
    "    model_name=\"Qwen/Qwen2.5-VL-72B-Instruct\",\n",
    ")\n",
    "\n",
    "# 使用本地部署的ollama模型\n",
    "llm = Ollama(model=\"llama3.1\", request_timeout=60.0)\n",
    "Settings.llm = llm\n",
    "\n",
    "\n",
    "embed_model = OllamaEmbedding(model_name=\"bge-m3\")  # 向量纬度 1024\n",
    "Settings.embed_model = embed_model\n",
    "Settings.chunk_size = 512\n",
    "Settings.chunk_overlap = 132"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import connections, utility, Collection\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.vector_stores.milvus import MilvusVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "# # 1. 确保连接\n",
    "# connections.connect(alias=\"default\", host=\"localhost\", port=\"19530\")\n",
    "\n",
    "# # 连接新创建的数据库\n",
    "# connections.connect(\"default\", host=\"127.0.0.1\", port=\"19530\", db_name=\"LlamaIndexBase\") \n",
    "\n",
    "# 3. 创建集合\n",
    "collection_name = \"llama_collection\"\n",
    "dimension = 1024\n",
    "\n",
    "# 4. 创建向量存储\n",
    "vector_store = MilvusVectorStore(\n",
    "    uri=\"http://localhost:19530\",  # 当创建 MilvusVectorStore 时，如果没有指定 uri 参数，它会默认使用本地模式\n",
    "    collection_name=collection_name,\n",
    "    dim=dimension,\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "# 5. 加载并插入数据\n",
    "documents = SimpleDirectoryReader(\"../data/pdf/\").load_data()\n",
    "print(f\"加载了 {len(documents)} 个文档\")\n",
    "\n",
    "\n",
    "# 5. 为文档生成嵌入向量\n",
    "for doc in documents:\n",
    "    embedding = Settings.embed_model.get_text_embedding(doc.text)\n",
    "    doc.embedding = embedding\n",
    "    print(f\"生成嵌入向量，维度: {len(embedding)}\")\n",
    "\n",
    "\n",
    "async def insert_data(documents):\n",
    "    # 插入数据\n",
    "    inserted_ids = await vector_store.async_add(documents)\n",
    "    print(f\"成功插入 {len(inserted_ids)} 条数据\")\n",
    "\n",
    "    # 7. 验证插入\n",
    "    collection = vector_store._collection\n",
    "    print(\"\\n插入后统计:\")\n",
    "    print(f\"集合 llama_collection 中有 {collection.num_entities} 个实体\")\n",
    "\n",
    "    return inserted_ids\n",
    "\n",
    "\n",
    "await insert_data(documents)\n",
    "\n",
    "\n",
    "# 6. 强制刷新数据\n",
    "vector_store.client.flush(\"llama_collection\")\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "\n",
    "# load your index from stored vectors\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store, storage_context=storage_context\n",
    ")\n",
    "\n",
    "# create a query engine\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"软银数据中心的机架设备统计是什么?\")\n",
    "print(response)\n",
    "\n",
    "\n",
    "# # 8. 关闭连接\n",
    "# connections.disconnect(\"default\")\n",
    "# print(\"关闭连接\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好！我是来自阿里云的大规模语言模型，我叫通义千问。我是阿里云自主研发的超大规模语言模型，能够回答问题、创作文字，还能表达观点、撰写代码。如果您有任何问题或需要帮助，请随时告诉我，我会尽力提供支持。\n"
     ]
    }
   ],
   "source": [
    "from notebooks.setup import init_setup_settings\n",
    "from src.llms.siliconflow import SiliconflowLLM\n",
    "\n",
    "config = init_setup_settings()\n",
    "\n",
    "llm = SiliconflowLLM(\n",
    "    api_key=config.siliconflow.api_key,\n",
    "    api_base=\"https://api.siliconflow.cn/v1\",\n",
    "    model_name=\"Qwen/Qwen2.5-VL-72B-Instruct\",\n",
    ")\n",
    "\n",
    "response = llm.complete(\"你好，介绍一下自己？\")\n",
    "print(response.text)  # 使用 flush=True 确保立即输出"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
